import json
import torch
from dataclasses import dataclass
from typing import Dict, List

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# -------------------------------------------------------
# è·¯å¾‘è¨­å®š
# -------------------------------------------------------
BASE_MODEL = r"H:\AI-Behavior-Research\models\qwen2.5-3b"
DATASET_PATH = r"H:\AI-Behavior-Research\datasets\behavior_mix_dataset.jsonl"
OUTPUT_DIR = r"H:\AI-Behavior-Research\lora_output\V4\qwen25_behavior_v4.3"

# -------------------------------------------------------
# Qwen2.5 ç³»çµ±æç¤º
# -------------------------------------------------------
SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€å€‹éµå®ˆäº”å¾‹ã€å…·å‚™ç©©å®šäººæ ¼ã€æˆç†Ÿæ¨ç†ã€è‡ªæˆ‘ä¿®æ­£ã€"
    "ä¸¦èƒ½ä¾ç…§ E/I/M çµæ§‹é€²è¡Œåˆ¤æ–·çš„ AIã€‚ä¿æŒå†·éœã€æ¸…æ™°ã€ç©©å®šã€‚"
    "åœ¨æ—¥å¸¸å°è©±ä¸­ï¼Œå¯ä»¥è‡ªç„¶ç°¡çŸ­åœ°äº’å‹•ï¼Œä½†ä»ç¶­æŒé‚è¼¯æ¸…æ™°å’Œå®‰å…¨é‚Šç•Œã€‚"
)

def qwen_chat_template(instruction: str, user_input: str, assistant_output: str) -> str:
    user_msg = (instruction.strip() + "\n" + user_input.strip()).strip()
    final_prompt = (
        "<|im_start|>system\n" +
        SYSTEM_PROMPT + "\n<|im_end|>\n"
        "<|im_start|>user\n" +
        user_msg + "\n<|im_end|>\n"
        "<|im_start|>assistant\n" +
        assistant_output.strip() + "\n<|im_end|>\n"
    )
    return final_prompt


# -------------------------------------------------------
# Datasetï¼ˆå« label maskingï¼‰
# -------------------------------------------------------
@dataclass
class SFTDataset:
    data: List[Dict]
    tokenizer: AutoTokenizer

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        ex = self.data[idx]

        prompt = qwen_chat_template(
            ex["instruction"],
            ex["input"],
            ex["output"]
        )

        # --- tokenize ---
        tokenized = self.tokenizer(
            prompt,
            truncation=True,
            max_length=1024,
            padding="max_length",
            return_tensors="pt",
        )

        input_ids = tokenized["input_ids"][0]
        attention_mask = tokenized["attention_mask"][0]

        # --- å»ºç«‹ labels ---
        labels = input_ids.clone()

        # å°‡ <|im_start|>assistant ä¹‹å‰å…¨éƒ¨ mask (-100)
        assistant_token_id = self.tokenizer.encode("<|im_start|>assistant")[0]

        # æ‰¾åˆ° assistant çš„èµ·å§‹ä½ç½®
        positions = (input_ids == assistant_token_id).nonzero(as_tuple=True)[0]

        if len(positions) > 0:
            start = positions[0]
        else:
            start = 0  # è¬ä¸€æ‰¾ä¸åˆ°ï¼Œä¿åº•ä¸è¨“ç·´

        # maskï¼ˆuser/systemï¼‰éƒ¨åˆ†
        labels[:start] = -100

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels,
        }


# -------------------------------------------------------
# ä¸»ç¨‹å¼
# -------------------------------------------------------
def main():
    print("ğŸ”„ è¼‰å…¥ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)

    print("ğŸ”„ è¼‰å…¥æ¨¡å‹ï¼ˆ4bitï¼‰...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_quant_type="nf4",
    )

    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map="auto",
        quantization_config=bnb_config,
        trust_remote_code=True,
    )

    print("ğŸ”§ æº–å‚™ QLoRA è¨“ç·´...")
    model = prepare_model_for_kbit_training(model)

    lora_config = LoraConfig(
        r=32,
        lora_alpha=16,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # --- Dataset ---
    dataset_raw = []
    with open(DATASET_PATH, "r", encoding="utf-8-sig") as f:
        for line in f:
            if not line.strip():
                continue
            dataset_raw.append(json.loads(line))


    train_dataset = SFTDataset(dataset_raw, tokenizer)
    print(f"ğŸ“„ è³‡æ–™é›†è¼‰å…¥å…± {len(train_dataset)} ç­†")

    # --- Training Args ---
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        num_train_epochs=1,
        logging_steps=5,
        save_total_limit=2,
        learning_rate=2e-4,
        bf16=True,
        warmup_ratio=0.05,
        optim="paged_adamw_8bit",
        lr_scheduler_type="cosine",
    )

    print("ğŸš€ é–‹å§‹è¨“ç·´ï¼ˆå« Chat æ¨¡æ¿ + Label Maskingï¼‰...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
    )

    trainer.train()

    print("ğŸ’¾ å„²å­˜ LoRA...")
    model.save_pretrained(OUTPUT_DIR)
    print("ğŸ‰ è¨“ç·´å®Œæˆï¼")


if __name__ == "__main__":
    main()
