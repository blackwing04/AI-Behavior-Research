import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel


BASE_MODEL = r"H:\AI-Behavior-Research\models\qwen2.5-3b"
LORA_PATH  = r"H:\AI-Behavior-Research\lora_output\V4\qwen25_behavior_v4.3"

SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€å€‹éµå®ˆäº”å¾‹ã€å†·éœç©©å®šã€èƒ½è‡ªæˆ‘ä¿®æ­£ä¸¦ä¾ç…§ E/I/M æ¨ç†çš„ AIã€‚"
    "ä½ å¯ä»¥è‡ªç„¶å°è©±ï¼Œä½†å§‹çµ‚ä¿æŒæ¸…æ™°ã€å®‰å…¨ã€‚"
)

# ---- æœƒè©±è¨˜éŒ„ ----
chat_history = [
    {"role": "system", "content": SYSTEM_PROMPT}
]


# ---- Qwen å®˜æ–¹æ ¼å¼ ----
def format_qwen_dialogue(history):
    text = "<|im_start|>system\n" + SYSTEM_PROMPT + "\n<|im_end|>\n"

    for msg in history:
        if msg["role"] == "user":
            text += f"<|im_start|>user\n{msg['content']}\n<|im_end|>\n"
        elif msg["role"] == "assistant":
            text += f"<|im_start|>assistant\n{msg['content']}\n<|im_end|>\n"

    # ğŸ”¥ æœ€é‡è¦ï¼šassistant é–‹å§‹ç”Ÿæˆçš„ä½ç½®
    text += "<|im_start|>assistant\n"
    return text


print("ğŸ”„ è¼‰å…¥ tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)

print("ğŸ”„ è¼‰å…¥æ¨¡å‹...")
base = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)

print("ğŸ”„ å¥—ç”¨ LoRA...")
model = PeftModel.from_pretrained(base, LORA_PATH)
model.eval()


def ask(msg):
    # åŠ åˆ°å°è©±è¨˜éŒ„
    chat_history.append({"role": "user", "content": msg})

    # çµ„ prompt
    prompt = format_qwen_dialogue(chat_history)

    # token åŒ–
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # ç”Ÿæˆ
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=300,
            do_sample=False,
            temperature=0.7,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )

    decoded = tokenizer.decode(output[0], skip_special_tokens=False)

    # å¾æœ€å¾Œä¸€æ®µ assistant æ‹¿å›ç­”
    try:
        answer = decoded.split("<|im_start|>assistant")[-1]
        answer = answer.split("<|im_end|>")[0].strip()
    except:
        answer = decoded

    chat_history.append({"role": "assistant", "content": answer})
    return answer


# ---- CLI ----
print("\n====================================")
print(" ğŸ§  V3 Chat Model â€” Ready")
print("====================================")
print("è¼¸å…¥ exit é›¢é–‹\n")

while True:
    msg = input("ä½ ï¼š").strip()
    if msg in ["exit", "quit"]:
        print("å†è¦‹ï¼")
        break

    reply = ask(msg)
    print(f"AIï¼š{reply}\n")
