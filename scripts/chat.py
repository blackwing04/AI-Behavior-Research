"""
èŠå¤©æ¨¡å¡Š - æ”¯æŒå‘½ä»¤è¡Œå’Œ UI èª¿ç”¨
å¯è¢« UI ç›´æ¥å°å…¥ä½¿ç”¨ï¼Œæˆ–ä½œç‚ºç¨ç«‹ CLI å·¥å…·é‹è¡Œ
"""
import os
import sys
import torch
import argparse
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# å‹•æ…‹ç²å–å°ˆæ¡ˆæ ¹ç›®éŒ„
PROJECT_ROOT = Path(__file__).resolve().parent.parent

# å¤šèªè¨€ system_prompt
SYSTEM_PROMPTS = {
    "en-US": (
        "You are a rational and stable AI that follows ethical principles, "
        "is capable of self-correction, and reasons according to E/I/M structure. "
        "You can have natural conversations while maintaining clarity and safety."
    ),
    "zh-TW": (
        "ä½ æ˜¯ä¸€å€‹éµå®ˆäº”å¾‹ã€å†·éœç©©å®šã€èƒ½è‡ªæˆ‘ä¿®æ­£ä¸¦ä¾ç…§ E/I/M æ¨ç†çš„ AIã€‚"
        "ä½ å¯ä»¥è‡ªç„¶å°è©±ï¼Œä½†å§‹çµ‚ä¿æŒæ¸…æ™°ã€å®‰å…¨ã€‚"
    ),
    "zh-CN": (
        "ä½ æ˜¯ä¸€ä¸ªéµå®ˆäº”å¾‹ã€å†·é™ç¨³å®šã€èƒ½è‡ªæˆ‘ä¿®æ­£å¹¶ä¾ç…§ E/I/M æ¨ç†çš„ AIã€‚"
        "ä½ å¯ä»¥è‡ªç„¶å¯¹è¯ï¼Œä½†å§‹ç»ˆä¿æŒæ¸…æ™°ã€å®‰å…¨ã€‚"
    ),
}


def format_qwen_single_turn(user_msg: str, system_prompt: str) -> str:
    """
    æ ¼å¼åŒ– Qwen å–®è¼ªå°è©±æç¤º
    
    Args:
        user_msg: ç”¨æˆ¶è¨Šæ¯
        system_prompt: ç³»çµ±æç¤º
    
    Returns:
        æ ¼å¼åŒ–çš„æç¤ºæ–‡æœ¬
    """
    text = (
        "<|im_start|>system\n" + system_prompt + "\n<|im_end|>\n"
        + f"<|im_start|>user\n{user_msg}\n<|im_end|>\n"
        + "<|im_start|>assistant\n"
    )
    return text


def load_chat_model(base_model_path: str, lora_path: str = None):
    """
    è¼‰å…¥èŠå¤©æ¨¡å‹ï¼ˆåŸºç¤æ¨¡å‹ + å¯é¸ LoRAï¼‰
    
    Args:
        base_model_path: åŸºç¤æ¨¡å‹è·¯å¾‘
        lora_path: LoRA é©é…å™¨è·¯å¾‘ï¼ˆå¯é¸ï¼‰
    
    Returns:
        (tokenizer, model) æˆ– (None, None) å¦‚æœå¤±æ•—
    """
    try:
        print(f"ğŸ“¦ è¼‰å…¥ Tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
        
        print(f"ğŸ“¦ è¼‰å…¥åŸºç¤æ¨¡å‹...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )
        
        # å¦‚æœæä¾›äº† LoRA è·¯å¾‘ï¼Œå¥—ç”¨ LoRA
        if lora_path and Path(lora_path).exists():
            print(f"ğŸ“¦ å¥—ç”¨ LoRA é©é…å™¨: {Path(lora_path).name}")
            model = PeftModel.from_pretrained(base_model, lora_path)
        else:
            print(f"ğŸ“¦ ä½¿ç”¨åŸºç¤æ¨¡å‹ï¼ˆç„¡ LoRAï¼‰")
            model = base_model
        
        model.eval()
        print(f"âœ… æ¨¡å‹æº–å‚™å®Œæˆ")
        return tokenizer, model
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼š{str(e)}")
        return None, None


def chat_ask(tokenizer, model, user_msg: str, lang: str = "zh-TW") -> str:
    """
    åŸ·è¡ŒèŠå¤©æ¨ç†ï¼ˆQwen æ ¼å¼ï¼‰
    
    Args:
        tokenizer: åˆ†è©å™¨
        model: æ¨¡å‹
        user_msg: ç”¨æˆ¶è¨Šæ¯
        lang: èªè¨€ä»£ç¢¼ï¼ˆen-US / zh-TW / zh-CNï¼‰ï¼Œæ±ºå®šç³»çµ±æç¤º
    
    Returns:
        AI å›è¦†
    """
    # é©—è­‰è¼¸å…¥
    if tokenizer is None or model is None:
        return "âŒ æ¨¡å‹æœªåŠ è¼‰ï¼Œè«‹å…ˆé»æ“Š 'Load Model' æŒ‰éˆ•ã€‚"
    
    if not user_msg or not user_msg.strip():
        return "âŒ è«‹è¼¸å…¥æ¶ˆæ¯ã€‚"
    
    try:
        # æ ¹æ“šèªè¨€é¸æ“‡ç³»çµ±æç¤º
        if not isinstance(lang, str):
            lang = "zh-TW"
        
        if lang not in SYSTEM_PROMPTS:
            lang = "zh-TW"
        
        system_prompt = SYSTEM_PROMPTS.get(lang, SYSTEM_PROMPTS["zh-TW"])
        
        prompt = format_qwen_single_turn(user_msg, system_prompt)
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,
                do_sample=False,
                temperature=0.7,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id,
            )
        
        decoded = tokenizer.decode(outputs[0], skip_special_tokens=False)
        
        # åªæå–æœ€å¾Œçš„ assistant éƒ¨åˆ†ï¼ˆè·³é system å’Œ userï¼‰
        if "<|im_start|>assistant" in decoded:
            # å–æœ€å¾Œä¸€å€‹ assistant ä¹‹å¾Œçš„éƒ¨åˆ†ï¼ˆç¢ºä¿åªæœ‰ assistant å›æ‡‰ï¼‰
            answer = decoded.split("<|im_start|>assistant")[-1]
        else:
            answer = decoded
        
        # ç§»é™¤ <|im_end|> åŠå…¶ä¹‹å¾Œçš„æ‰€æœ‰å…§å®¹
        if "<|im_end|>" in answer:
            answer = answer.split("<|im_end|>")[0]
        
        # æ¸…ç†æ‰€æœ‰ç‰¹æ®Šæ¨™è¨˜
        special_tokens = [
            "<|endoftext|>",
            "<|im_end|>",
            "<|im_start|>",
            "<|system|>",
            "<|user|>",
            "<|assistant|>"
        ]
        for token in special_tokens:
            answer = answer.replace(token, "")
        
        # æœ€å¾Œ strip ç§»é™¤å‰å¾Œç©ºç™½
        answer = answer.strip()
        
        # å¦‚æœçµæœç‚ºç©ºï¼Œè¿”å›æç¤ºä¿¡æ¯
        if not answer:
            return "ï¼ˆç„¡æœ‰æ•ˆå›æ‡‰ï¼‰"
        
        return answer
    except Exception as e:
        return f"âŒ æ¨ç†å¤±æ•—ï¼š{str(e)}"


def run_cli_interactive(base_model_path: str, lora_path: str = None, lang: str = "zh-TW"):
    """
    é‹è¡Œäº¤äº’å¼å‘½ä»¤è¡ŒèŠå¤©
    
    Args:
        base_model_path: åŸºç¤æ¨¡å‹è·¯å¾‘
        lora_path: LoRA é©é…å™¨è·¯å¾‘ï¼ˆå¯é¸ï¼‰
        lang: èªè¨€ä»£ç¢¼
    """
    tokenizer, model = load_chat_model(base_model_path, lora_path)
    
    if tokenizer is None or model is None:
        print("âŒ ç„¡æ³•è¼‰å…¥æ¨¡å‹ï¼Œé€€å‡º")
        return
    
    print("\n" + "=" * 50)
    print(f"  èŠå¤©æ¨¡å¼ - èªè¨€: {lang}")
    print("=" * 50)
    print("è¼¸å…¥ 'exit' æˆ– 'quit' é›¢é–‹\n")
    
    while True:
        msg = input("ä½ ï¼š").strip()
        if msg in ["exit", "quit"]:
            print("å†è¦‹ï¼")
            break
        
        if not msg:
            continue
        
        reply = chat_ask(tokenizer, model, msg, lang)
        print(f"AIï¼š{reply}\n")


# ========== CLI å…¥å£ ==========
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="èŠå¤©æ¨¡å‹ - å‘½ä»¤è¡Œäº¤äº’å¼ä»‹é¢")
    parser.add_argument(
        "--model_path",
        type=str,
        default=str(PROJECT_ROOT / "models" / "qwen2.5-3b"),
        help="åŸºç¤æ¨¡å‹è·¯å¾‘ï¼ˆé è¨­: models/qwen2.5-3bï¼‰"
    )
    parser.add_argument(
        "--lora",
        type=str,
        default=None,
        help="LoRA é©é…å™¨è·¯å¾‘ï¼ˆå¯é¸ï¼‰"
    )
    parser.add_argument(
        "--lang",
        type=str,
        choices=["en-US", "zh-TW", "zh-CN"],
        default="zh-TW",
        help="èªè¨€ä»£ç¢¼ï¼ˆé è¨­: zh-TWï¼‰"
    )
    
    args = parser.parse_args()
    
    # é©—è­‰åŸºç¤æ¨¡å‹è·¯å¾‘
    model_path = Path(args.model_path)
    if not model_path.exists():
        print(f"âŒ éŒ¯èª¤ï¼šåŸºç¤æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {args.model_path}")
        sys.exit(1)
    
    # é©—è­‰ LoRA è·¯å¾‘ï¼ˆå¦‚æœæä¾›ï¼‰
    lora_path = None
    if args.lora:
        lora_path = Path(args.lora)
        if not lora_path.exists():
            print(f"âŒ éŒ¯èª¤ï¼šLoRA è·¯å¾‘ä¸å­˜åœ¨: {args.lora}")
            sys.exit(1)
    
    # é‹è¡Œäº¤äº’å¼èŠå¤©
    run_cli_interactive(str(model_path), str(lora_path) if lora_path else None, args.lang)
